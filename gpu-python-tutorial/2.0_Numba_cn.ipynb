{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "05db90c8-a261-49ec-a4a2-c97cb8013ec7",
      "metadata": {},
      "source": [
        "# Numba\n",
        "\n",
        "[Numba](https://numba.pydata.org/) 是一个开源的 JIT 编译器，能够将 Python 和 NumPy 代码的子集转换为快速的机器码。\n",
        "\n",
        "Numba 通过直接将受限的 Python 代码子集编译为 CUDA 内核和设备函数来支持 CUDA GPU 编程，遵循 CUDA 执行模型。用 Numba 编写的内核似乎可以直接访问 NumPy 数组。NumPy 数组会在 CPU 和 GPU 之间自动传输。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1df11e1b-234c-4008-bf0c-bbdb897e0f07",
      "metadata": {},
      "source": [
        "## 什么是内核？\n",
        "\n",
        "内核类似于函数，它是一个代码块，接受一些输入并由处理器执行。\n",
        "\n",
        "函数和内核之间的区别是：\n",
        "- 内核不能返回任何内容，必须修改内存\n",
        "- 内核必须指定其线程层次结构（线程和块）"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "412bb30f-31d3-41b5-bec7-9d7dd72472fc",
      "metadata": {},
      "source": [
        "## 什么是网格、线程和块（以及线程束）？\n",
        "\n",
        "[线程和块](https://en.wikipedia.org/wiki/Thread_block_(CUDA_programming)) 是你指示 GPU 并行处理某些代码的方式。我们的 GPU 是一个并行处理器，所以我们需要指定希望内核执行的次数。\n",
        "\n",
        "线程的好处是它们之间有一些共享的缓存内存，但每个 GPU 上的核心数量有限，所以我们需要将工作分解成块，这些块将在 GPU 上并行调度和运行。\n",
        "\n",
        "<figure>\n",
        "\n",
        "![CPU GPU 对比](images/threads-blocks-warps.png)\n",
        "\n",
        "<figcaption style=\"text-align: center;\"> \n",
        "    \n",
        "图片来源 <a href=\"https://docs.nvidia.com/cuda/cuda-c-programming-guide/\">https://docs.nvidia.com/cuda/cuda-c-programming-guide/</a>\n",
        "    \n",
        "</figcaption>\n",
        "</figure>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fad6a21f-ce9c-49a2-9b6a-41fbd29bf6d1",
      "metadata": {},
      "source": [
        "### 什么？？\n",
        "\n",
        "现在不要太担心这个。只需要记住**我们需要指定希望内核被调用的次数**，这是通过两个数字相乘来给出整体网格大小的。\n",
        "\n",
        "选择每个块的线程数的经验法则：\n",
        "- 应该是线程束大小（32）的倍数\n",
        "- 一个好的起点是每个块 128-512 个线程，但需要通过基准测试来确定最佳值。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d36336db-8514-4b21-8072-62f085779f93",
      "metadata": {},
      "source": [
        "## Hello world"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8dded05-6f39-4ea3-8ac6-45247ebc35d1",
      "metadata": {},
      "source": [
        "让我们通过一些代码深入了解，希望事情会变得更清晰。\n",
        "\n",
        "首先，让我们编写一个简单的基于 CPU 的 Python 函数，我们将在[列表推导式](https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions)中重复调用它。从 Python 的角度来看，列表推导式可以是并行计算的一个很好的起点，因为它们本身已经感觉有些并行性。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f4a31223-019a-403b-bbd9-c3daa2f1042f",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = range(10)\n",
        "\n",
        "def foo(i):\n",
        "    return i\n",
        "    \n",
        "[foo(i) for i in data]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c420044-2d0a-4ca6-8643-82f96390ff2a",
      "metadata": {},
      "source": [
        "这里我们的 `foo` 函数返回其索引值，并使用 `for` 循环遍历由 `range` 生成的数据。\n",
        "\n",
        "接下来，我们将把它转换为 CUDA 内核，并使用 Numba CUDA 在我们的 GPU 上运行它。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81a1d262-f928-4473-b6b1-450bcb9fe60b",
      "metadata": {},
      "source": [
        "首先我们需要记住，我们的内核不能返回任何内容。相反，我们将使用一个输出列表来存储我们想要返回的值。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ea8d3d9c-72fe-4ff7-b9f9-44e23dcf258e",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = range(10)\n",
        "output = []\n",
        "\n",
        "def foo(i):\n",
        "    output.append(i)\n",
        "    \n",
        "[foo(i) for i in data]\n",
        "\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d161843-ee8d-4cb0-bf02-481c61b8b4e0",
      "metadata": {},
      "source": [
        "我们的下一个挑战是 GPU 上的输出数组必须具有固定长度。我们不能从一个空数组开始并不断追加内容。所以让我们使用 NumPy 创建一个与输入数据长度相同的数组。我们还将输入列表转换为 NumPy 数组，因为这是我们可以移动到 GPU 的内容。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "099b300d-0f4d-4f05-bbc7-b6b0e888f226",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d6832e81-5a23-4af2-bdd1-c562a21e6d33",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = np.asarray(range(10))\n",
        "output = np.zeros(len(data))\n",
        "\n",
        "def foo(i):\n",
        "    output[i] = i\n",
        "    \n",
        "[foo(i) for i in data]\n",
        "\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15a42ff5-cc99-4889-acc5-443819664fdd",
      "metadata": {},
      "source": [
        "现在我们的纯 Python 函数行为类似于内核，让我们使用 Numba 将其转换为真正的内核。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "16577800",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['NUMBA_CUDA_USE_NVIDIA_BINDING'] = '1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e60e9835-253f-4786-be3a-780f08df89da",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from numba import cuda\n",
        "from numba import config as numba_config\n",
        "numba_config.CUDA_ENABLE_PYNVJITLINK = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "edd640a5-ceaf-4fc1-9d87-9f9bda5d1d87",
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/miniconda3/envs/acccom/lib/python3.12/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        },
        {
          "ename": "LinkerError",
          "evalue": "[222] Call to cuLinkAddData results in CUDA_ERROR_UNSUPPORTED_PTX_VERSION\nptxas application ptx input, line 9; fatal   : Unsupported .version 8.4; current version is '8.2'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mCudaAPIError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/acccom/lib/python3.12/site-packages/numba/cuda/cudadrv/driver.py:2830\u001b[39m, in \u001b[36mCtypesLinker.add_ptx\u001b[39m\u001b[34m(self, ptx, name)\u001b[39m\n\u001b[32m   2829\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2830\u001b[39m     \u001b[43mdriver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuLinkAddData\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menums\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCU_JIT_INPUT_PTX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2831\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mptxbuf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mptx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamebuf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2832\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m CudaAPIError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/acccom/lib/python3.12/site-packages/numba/cuda/cudadrv/driver.py:327\u001b[39m, in \u001b[36mDriver._ctypes_wrap_fn.<locals>.safe_cuda_api_call\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m    326\u001b[39m retcode = libfn(*args)\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_ctypes_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretcode\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/acccom/lib/python3.12/site-packages/numba/cuda/cudadrv/driver.py:395\u001b[39m, in \u001b[36mDriver._check_ctypes_error\u001b[39m\u001b[34m(self, fname, retcode)\u001b[39m\n\u001b[32m    394\u001b[39m     \u001b[38;5;28mself\u001b[39m._detect_fork()\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m CudaAPIError(retcode, msg)\n",
            "\u001b[31mCudaAPIError\u001b[39m: [222] Call to cuLinkAddData results in CUDA_ERROR_UNSUPPORTED_PTX_VERSION",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mLinkerError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m     i = cuda.grid(\u001b[32m1\u001b[39m)\n\u001b[32m      7\u001b[39m     output_array[i] = input_array[i]\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mfoo\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m output\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/acccom/lib/python3.12/site-packages/numba/cuda/dispatcher.py:539\u001b[39m, in \u001b[36m_LaunchConfiguration.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    538\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args):\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdispatcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgriddim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblockdim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m                                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msharedmem\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/acccom/lib/python3.12/site-packages/numba/cuda/dispatcher.py:681\u001b[39m, in \u001b[36mCUDADispatcher.call\u001b[39m\u001b[34m(self, args, griddim, blockdim, stream, sharedmem)\u001b[39m\n\u001b[32m    679\u001b[39m     kernel = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m.overloads.values()))\n\u001b[32m    680\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m681\u001b[39m     kernel = \u001b[43m_dispatcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDispatcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_cuda_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    683\u001b[39m kernel.launch(args, griddim, blockdim, stream, sharedmem)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/acccom/lib/python3.12/site-packages/numba/cuda/dispatcher.py:689\u001b[39m, in \u001b[36mCUDADispatcher._compile_for_args\u001b[39m\u001b[34m(self, *args, **kws)\u001b[39m\n\u001b[32m    687\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kws\n\u001b[32m    688\u001b[39m argtypes = [\u001b[38;5;28mself\u001b[39m.typeof_pyval(a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[32m--> \u001b[39m\u001b[32m689\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margtypes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/acccom/lib/python3.12/site-packages/numba/cuda/dispatcher.py:934\u001b[39m, in \u001b[36mCUDADispatcher.compile\u001b[39m\u001b[34m(self, sig)\u001b[39m\n\u001b[32m    932\u001b[39m     kernel = _Kernel(\u001b[38;5;28mself\u001b[39m.py_func, argtypes, **\u001b[38;5;28mself\u001b[39m.targetoptions)\n\u001b[32m    933\u001b[39m     \u001b[38;5;66;03m# We call bind to force codegen, so that there is a cubin to cache\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m934\u001b[39m     \u001b[43mkernel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    935\u001b[39m     \u001b[38;5;28mself\u001b[39m._cache.save_overload(sig, kernel)\n\u001b[32m    937\u001b[39m \u001b[38;5;28mself\u001b[39m.add_overload(kernel, argtypes)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/acccom/lib/python3.12/site-packages/numba/cuda/dispatcher.py:197\u001b[39m, in \u001b[36m_Kernel.bind\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    194\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    195\u001b[39m \u001b[33;03m    Force binding to current CUDA context\u001b[39;00m\n\u001b[32m    196\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_codelibrary\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_cufunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/acccom/lib/python3.12/site-packages/numba/cuda/codegen.py:214\u001b[39m, in \u001b[36mCUDACodeLibrary.get_cufunc\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cufunc:\n\u001b[32m    212\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cufunc\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m cubin = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_cubin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_capability\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    215\u001b[39m module = ctx.create_module_image(cubin)\n\u001b[32m    217\u001b[39m \u001b[38;5;66;03m# Load\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/acccom/lib/python3.12/site-packages/numba/cuda/codegen.py:188\u001b[39m, in \u001b[36mCUDACodeLibrary.get_cubin\u001b[39m\u001b[34m(self, cc)\u001b[39m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     ptx = \u001b[38;5;28mself\u001b[39m.get_asm_str(cc=cc)\n\u001b[32m--> \u001b[39m\u001b[32m188\u001b[39m     \u001b[43mlinker\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_ptx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mptx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._linking_files:\n\u001b[32m    191\u001b[39m     linker.add_file_guess_ext(path)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/acccom/lib/python3.12/site-packages/numba/cuda/cudadrv/driver.py:2833\u001b[39m, in \u001b[36mCtypesLinker.add_ptx\u001b[39m\u001b[34m(self, ptx, name)\u001b[39m\n\u001b[32m   2830\u001b[39m     driver.cuLinkAddData(\u001b[38;5;28mself\u001b[39m.handle, enums.CU_JIT_INPUT_PTX,\n\u001b[32m   2831\u001b[39m                          ptxbuf, \u001b[38;5;28mlen\u001b[39m(ptx), namebuf, \u001b[32m0\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   2832\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m CudaAPIError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m2833\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LinkerError(\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % (e, \u001b[38;5;28mself\u001b[39m.error_log))\n",
            "\u001b[31mLinkerError\u001b[39m: [222] Call to cuLinkAddData results in CUDA_ERROR_UNSUPPORTED_PTX_VERSION\nptxas application ptx input, line 9; fatal   : Unsupported .version 8.4; current version is '8.2'"
          ]
        }
      ],
      "source": [
        "data = np.asarray(range(10))\n",
        "output = np.zeros(len(data))\n",
        "\n",
        "@cuda.jit\n",
        "def foo(input_array, output_array):\n",
        "    i = cuda.grid(1)\n",
        "    output_array[i] = input_array[i]\n",
        "    \n",
        "foo[1, len(data)](data, output)\n",
        "\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "758f7db8-9673-4b04-a7b3-2b8ef0e21907",
      "metadata": {},
      "source": [
        "**太棒了，上面的代码在我们的 GPU 上运行了！**\n",
        "\n",
        "现在让我们来解析一下。\n",
        "\n",
        "要将我们的 CPU 函数转换为 GPU 内核，我们需要添加 `@cuda.jit` 装饰器。这告诉 Numba 在运行时将我们的代码编译为 CUDA 兼容的字节码。\n",
        "\n",
        "接下来，我们将内核的输入更改为 `input_array` 和 `output_array`。这是因为我们的内核需要引用这两个数组才能与它们交互。（稍后会详细介绍。）\n",
        "\n",
        "但是 `i` 呢？我们不再需要在每次调用函数时传递索引，而是可以依赖一个很好的 CUDA 函数 `cuda.grid`，它允许我们的内核在运行时获取自己的线程索引。\n",
        "\n",
        "最后我们做了一个看起来奇怪的函数调用 `foo[blocks, threads](input, output)`。为了在 GPU 上并行运行我们的内核，我们需要指定希望它运行的次数。内核函数使用方括号配置，传递块大小和线程大小。由于我们的数组只有 `10` 个元素长，我们指定块大小为 `1`，线程大小为 `10`，这意味着我们的内核将被执行 `10` 次。然后我们像往常一样传递参数。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1d83f9b-c4c7-4b18-b83f-21c85cb0a5de",
      "metadata": {},
      "source": [
        "## 稍微大一点的东西\n",
        "\n",
        "现在我们已经用 Numba 运行了第一个 CUDA 内核，让我们尝试一些稍微大一点的东西。\n",
        "\n",
        "这次我们将取一个大型数组，并将其中的每个数字加倍。我们首先在 CPU 上用纯 Python 实现，然后在 GPU 上用 CUDA 内核实现。\n",
        "\n",
        "让我们从一个包含 3000 万个随机数的大型数组开始，以及一个等长的输出数组。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d06dc1c-b36e-48df-9c7f-0bf109c1ceae",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "random_array = np.random.random((30_000_000))\n",
        "random_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2791c50f-a80b-4ccc-86a1-5955c508dc78",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "output = np.zeros_like(random_array)\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d90ac1c-932a-4345-9e70-8a2e24f15c10",
      "metadata": {},
      "source": [
        "然后在 Python 中，让我们遍历这个数组，并将每个项加倍到输出数组中。我们可以计时这个单元格，看看需要多长时间。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "195b97db-2efc-4e64-8f95-3ade92c52a07",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "def foo(i):\n",
        "    output[i] = random_array[i] * 2\n",
        "    \n",
        "[foo(i) for i in range(len(random_array))]\n",
        "\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d881d061-b924-48b5-97d9-8ce5bc862e69",
      "metadata": {},
      "source": [
        "对我来说，CPU 完成这个计算大约需要 10 秒。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "966920e3-653d-458c-9f9b-1e740e1db024",
      "metadata": {},
      "source": [
        "接下来，让我们编写一个 CUDA 内核，做完全相同的事情。与上一个示例的唯一区别是，我们将线程大小设置为固定值 `128`，然后计算需要多少个块来覆盖整个数组。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c98f0967-ad98-420f-9ccb-1eae9e242bc7",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9723dc8a-9627-442f-becc-3d9fc5423096",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "output = np.zeros_like(random_array)\n",
        "\n",
        "threads = 128\n",
        "blocks = math.ceil(random_array.shape[0] / threads)\n",
        "\n",
        "@cuda.jit\n",
        "def foo(input_array, output_array):\n",
        "    i = cuda.grid(1)\n",
        "    output_array[i] = input_array[i] * 2\n",
        "    \n",
        "foo[blocks, threads](random_array, output)\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abec8daa-6d42-46cb-b95e-9f896de3a69d",
      "metadata": {},
      "source": [
        "太棒了！现在速度快了几个数量级，只需要几百毫秒。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4043550-55eb-4a74-bdb7-32e97f58314e",
      "metadata": {},
      "source": [
        "精明的你可能会想，NumPy 已经是一个基于 C 的优化库，我们正在将 GPU 内核与一些纯 Python 代码进行比较。如果我们用 NumPy 实现会怎样？\n",
        "\n",
        "嗯，你是对的，在这个例子中，NumPy 仍然比我们的 GPU 快。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac1d6108-8d8f-4318-b7d8-f08cbf5c8b4d",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "%%time \n",
        "\n",
        "random_array * 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b5a9d0e-775a-4871-8416-f81e232e728e",
      "metadata": {},
      "source": [
        "但这种情况的原因是内存管理。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "383ba959-6e68-4f89-a887-8c139e847789",
      "metadata": {},
      "source": [
        "## 内存管理\n",
        "\n",
        "之前我们讨论过，CPU 和 GPU 实际上是两台独立的计算机。每台计算机都有自己的内存。\n",
        "\n",
        "到目前为止我们处理的所有数据都是在 CPU 上用 `numpy` 创建的。因此，为了让 `numba` 在 GPU 上处理这些数据，它一直在悄悄地为我们来回复制数据。\n",
        "\n",
        "这种数据移动会带来性能损失。\n",
        "\n",
        "我们也可以选择自己控制数据。我们可以使用 `cuda.to_device` 提前将数组显式移动到 GPU。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a96de9eb-2adc-4179-89ce-5be1bdc2fb6b",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "gpu_random_array = cuda.to_device(random_array)\n",
        "gpu_output = cuda.to_device(np.zeros_like(random_array))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dccdba5-b280-448e-9d26-98e0120b283d",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "gpu_random_array"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d552a86-08da-4324-bfc4-1c726aca528d",
      "metadata": {},
      "source": [
        "现在如果我们再次运行内核，并将 GPU 内存数组传递给它，我们会看到它确实比 NumPy 表现更好。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "651b7b29-4bad-4015-af25-1040f15b1946",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "%%timeit -n 100\n",
        "foo[blocks, threads](gpu_random_array, gpu_output)\n",
        "cuda.synchronize()  # 等待内核计算，因为它会延迟完成"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39df78d6-6ea7-4252-9bcd-203d0920a81b",
      "metadata": {},
      "source": [
        "然而我们的输出结果也仍然在 GPU 上。我们显式地将其复制到了那里，所以我们需要使用 `copy_to_host()` 显式地将其复制回来。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d1d93c1-f54d-4c4f-a789-237ff74409d9",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "gpu_output.copy_to_host()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6dc8ac8-433c-46e0-a978-065364089e18",
      "metadata": {},
      "source": [
        "这两种数据移动操作都需要时间。但我们在这里执行的计算是微不足道的。随着内核内的计算变得越来越复杂，复制数据所花费的时间比例会变得越来越小。\n",
        "\n",
        "内存管理在其他地方也很有用。我们可能希望编写代码，在其中编写许多内核并将它们链接在一起。在每个内核调用之间将数据复制到 GPU 并再次复制回来将是低效的。\n",
        "\n",
        "```python\n",
        "# 将数组移动到 GPU\n",
        "foo[blocks, threads](data, output)\n",
        "# 将数据移回 CPU\n",
        "\n",
        "# 将数组移动到 GPU\n",
        "bar[blocks, threads](data, output)\n",
        "# 将数据移回 CPU\n",
        "\n",
        "# 将数组移动到 GPU\n",
        "baz[blocks, threads](data, output)\n",
        "# 将数据移回 CPU\n",
        "```\n",
        "\n",
        "因此，通过显式地将数据放在那里，我们可以减少这个时间，并更好地控制我们的计算。\n",
        "\n",
        "```python\n",
        "# 将数组移动到 GPU\n",
        "data = cuda.to_device(data)\n",
        "output = cuda.to_device(output)\n",
        "\n",
        "foo[blocks, threads](data, output)\n",
        "bar[blocks, threads](data, output)\n",
        "baz[blocks, threads](data, output)\n",
        "\n",
        "# 将数据移回 CPU\n",
        "data = data.copy_to_host()\n",
        "output = output.copy_to_host()\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "acccom",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
